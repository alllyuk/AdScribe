# Классификатор изображений для определения количества предметов

## Описание задачи

Проект решает задачу классификации изображений обуви на три класса:
- **Один предмет** - на изображении один предмет обуви
- **Разнообразие предметов** - на изображении несколько предметов или комплект
- **Непонятно** - сложно определить количество предметов

## Структура проекта

```
/workspace/AAA_project/experimental/final_classifier/
├── meta_info_with_main_img.csv     # Исходный датасет
├── labeled_dataset.csv             # Размеченный датасет (создается автоматически)
├── labeling.py                     # Скрипт для автоматической разметки
├── train.py                        # Скрипт для обучения классификатора
├── requirements.txt                # Зависимости
├── README.md                       # Этот файл
├── model_checkpoint/               # Папка с чекпоинтами модели (создается автоматически)
├── image_cache/                    # Кэш изображений (создается автоматически)
└── confusion_matrix.png            # Матрица ошибок (создается автоматически)
```

## Установка зависимостей

```bash
pip install -r requirements.txt
```

## Использование

### 1. Создание разметки

Первый шаг - создание автоматической разметки на основе текстовых данных:

```bash
python labeling.py
```

Этот скрипт:
- Анализирует текстовые поля (title, description, other)
- Определяет количество предметов по ключевым словам
- Создает файл `labeled_dataset.csv` с разметкой
- Выводит статистику по классам

### 2. Обучение классификатора

Второй шаг - обучение нейронной сети:

```bash
python train.py
```

Этот скрипт:
- Загружает размеченный датасет
- Создает train/validation/test разбиения
- Обучает ResNet18 на изображениях
- Сохраняет лучшую модель в `model_checkpoint/`
- Выводит детальные метрики и графики

## Алгоритм разметки

Нейросетевая автоматическая разметка основана на передовых методах NLP:

### Этап 1: Создание эталонных эмбеддингов
- Используется предобученная модель `cointegrated/rubert-tiny2` для русского языка
- Создаются эталонные тексты для каждого класса
- Вычисляются центроидные эмбеддинги для каждого класса

### Этап 2: Семантический анализ текста
- **Эмбеддинги**: Sentence-BERT для получения векторных представлений текста
- **Расстояния**: Косинусное сходство между эмбеддингами текста и эталонами
- **Кластеризация**: K-means для автоматического обнаружения паттернов

### Этап 3: Мета-классификатор
- **Псевдометки**: Создание первичных меток на основе семантических расстояний
- **Гибридные признаки**: Комбинирование эмбеддингов и численных признаков
- **Random Forest**: Финальная классификация с учетом всех признаков

### Классы разметки:

#### Один предмет
- Высокое сходство с эталонами одного предмета
- Отсутствие множественных маркеров в тексте
- Уверенность на основе четкости семантического сигнала

#### Разнообразие предметов  
- Семантическая близость к текстам о комплектах
- Обнаружение паттернов множественности через эмбеддинги
- Численные признаки: "две пары", "комплект", "набор"

#### Непонятно
- Высокая семантическая неопределенность
- Большие расстояния до всех эталонов
- Паттерны неуверенности в тексте

## Архитектура модели

- **Базовая модель**: ResNet18 (предобученная на ImageNet)
- **Классификационная голова**: Linear layer для 3 классов
- **Регуляризация**: Dropout 0.5
- **Аугментация данных**: Flip, Rotation, ColorJitter

## Метрики оценки

Модель оценивается по:
- Accuracy
- Precision, Recall, F1-score для каждого класса
- Confusion Matrix
- Графики обучения (loss, accuracy)

## Результаты

После обучения создаются файлы:
- `model_checkpoint/best_model.pth` - лучшая модель
- `training_history.png` - графики обучения
- `confusion_matrix.png` - матрица ошибок
- Детальный отчет в консоли

## Особенности реализации

1. **Нейросетевые эмбеддинги** - использование предобученной модели ruBERT для глубокого понимания семантики

2. **Семантические расстояния** - измерение косинусного сходства между векторными представлениями

3. **Кластерный анализ** - автоматическое обнаружение групп похожих текстов

4. **Мета-классификатор** - комбинирование нескольких источников информации

5. **Псевдометки** - создание обучающих данных на основе семантического анализа

6. **Гибридные признаки** - объединение эмбеддингов и классических текстовых признаков

7. **Адаптивная уверенность** - оценка качества предсказаний на основе расстояний

## Адаптация под реальные данные

Для работы с реальными изображениями необходимо:
1. Изменить функцию `load_image_from_url()` в `train.py`
2. Настроить правильное получение URL изображений по ID
3. Возможно, добавить обработку разных форматов изображений

## Улучшения

Возможные направления для улучшения:
- Использование более мощных языковых моделей (ruBERT-large, ruGPT)
- Активное обучение для улучшения псевдометок
- Ансамблирование нескольких нейросетевых моделей
- Добавление контекстных эмбеддингов (BERT, T5)
- Использование графовых нейронных сетей для связей между объявлениями
- Few-shot learning для редких классов